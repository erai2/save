
import os
import re
import json
import pdfplumber
import pandas as pd
from pathlib import Path
from langchain.vectorstores import Chroma, FAISS
from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings
from langchain.docstore.document import Document

# ğŸ“„ ë¬¸ì„œ â†’ í…ìŠ¤íŠ¸
def extract_text(file_path):
    ext = Path(file_path).suffix.lower()
    if ext == ".pdf":
        with pdfplumber.open(file_path) as pdf:
            return "\n".join([page.extract_text() or "" for page in pdf])
    return Path(file_path).read_text(encoding="utf-8")

# ğŸ“˜ í…ìŠ¤íŠ¸ â†’ êµ¬ì¡°í™”
def parse_cases(text):
    pattern = re.split(r"(?:â—‰|<ì‚¬ë¡€\s*\d+>|#\s*ì‚¬ë¡€\s*\d+|ì‚¬ë¡€\s*\d+|ì˜ˆì‹œ\s*\d+|â– )", text)
    blocks = []
    for block in pattern:
        lines = block.strip().splitlines()
        if len(lines) < 2:
            continue
        title = lines[0].strip()
        body = "\n".join(lines[1:]).strip()
        summary = " / ".join([l for l in body.splitlines() if l.strip()][:5])
        category = "ì‚¬ë¡€" if "ì‚¬ë¡€" in title else "ê·œì¹™" if "ë²•ì¹™" in title else "ê¸°íƒ€"
        blocks.append({"ì œëª©": title, "ë‚´ìš©": body, "ìš”ì•½": summary, "êµ¬ë¶„": category})
    return blocks

# ğŸ’¾ ë²¡í„° ì €ì¥
def embed_and_save(blocks, db_dir, db_type, embed_type):
    docs = [Document(page_content=f"[{b['êµ¬ë¶„']}] {b['ì œëª©']}\n{b['ë‚´ìš©']}", metadata={"title": b["ì œëª©"]}) for b in blocks]
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2") if embed_type == "HF" else OpenAIEmbeddings()
    if db_type == "Chroma":
        db = Chroma.from_documents(docs, emb, persist_directory=db_dir)
        db.persist()
    elif db_type == "FAISS":
        db = FAISS.from_documents(docs, emb)
        FAISS.save_local(db, db_dir)
    return db

# ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰
def search_vector(db_dir, query, db_type, embed_type, k=3):
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2") if embed_type == "HF" else OpenAIEmbeddings()
    db = Chroma(persist_directory=db_dir, embedding_function=emb) if db_type == "Chroma" else FAISS.load_local(db_dir, emb)
    return db.similarity_search(query, k=k)

# ğŸ§  GPT ìš”ì•½ (ì˜µì…˜)
def gpt_summary(query, docs):
    import openai
    combined = "\n\n".join([doc.page_content[:1000] for doc in docs])
    prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}
ì•„ë˜ëŠ” ê´€ë ¨ëœ ì‚¬ì£¼ ì‚¬ë¡€ì…ë‹ˆë‹¤. ê²©êµ­, ì œì•• ë°©ì‹, í˜„ì‹¤ í•´ì„ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”:

{combined}
"""
    res = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
    )
    return res.choices[0].message.content.strip()

# âœ… ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    files = ["case.json", "DB.pdf"]  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì •
    all_blocks = []
    for file_path in files:
        print(f"ğŸ“‚ {file_path} ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        raw_text = extract_text(file_path)
        blocks = parse_cases(raw_text)
        all_blocks.extend(blocks)

    print(f"âœ… ì´ {len(all_blocks)}ê±´ êµ¬ì¡°í™”ë¨")
    df = pd.DataFrame(all_blocks)
    df.to_csv("saju_structured_all.csv", index=False, encoding="utf-8-sig")

    db_dir = "saju_vector_db"
    db_type = "Chroma"
    embed_type = "HF"

    print("ğŸ§  ë²¡í„° DB ìƒì„± ì¤‘...")
    embed_and_save(all_blocks, db_dir, db_type, embed_type)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {db_dir}")

    query = "ç”²æ—¥ì£¼ì—ì„œ ì‹ì‹ ì´ ì¬ë¥¼ ìƒí•˜ë©´?"
    print(f"ğŸ” ê²€ìƒ‰: {query}")
    results = search_vector(db_dir, query, db_type, embed_type, k=3)

    result_data = []
    for i, r in enumerate(results):
        print(f"\n{i+1}. {r.metadata['title']}\n{r.page_content[:500]}...")
        result_data.append({
            "ìˆœë²ˆ": i + 1,
            "ì œëª©": r.metadata["title"],
            "ë‚´ìš©": r.page_content[:1000]
        })

    pd.DataFrame(result_data).to_csv("search_results.csv", index=False, encoding="utf-8-sig")
    print("ğŸ“ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: search_results.csv")

    use_gpt = True
    if use_gpt:
        print("\nğŸ§  GPT ìš”ì•½ ê²°ê³¼:")
        print(gpt_summary(query, results))
